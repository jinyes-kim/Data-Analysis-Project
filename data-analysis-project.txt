직원 이탈에 영향을 주는 데이터

PCA - 사용불가 

단순히 주성분 분석이라기 보다는 주성분이 될 수 있는 형태로 기존의 데이터에 변환을 가하는
것이기 때문이다. 따라서 원래 가지고 있는 속성이 중요한 경우 PCA를 사용할 수 없다.


# 1. 로지스틱 회귀 스코어가 별 차이가 없는 점이 왜 그런지?
# 2. presicion, recall, f1 score의 의미
# 3. 



#code
"""
# IBM HR Data-Set

<process>
1. 데이터 로드
2. 데이터 탐색
3. 카테고리컬 데이터 시각화 & 셀렉션
4. 더미 변수로 추가하기 (절편에 영향을 준다)
5. 히트맵으로 상관계수 살펴보기
6. 상관계수 기반으로 피쳐 셀렉션
7. 로지스틱 회귀


"""


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import preprocessing

# Attrition 
# True == 1 False == 0

# load dataset
HR = pd.read_csv("C:/project-IBM/data/WA_Fn-UseC_-HR-Employee-Attrition.csv")
HR.index.name = 'record'
HR.info()
HR.describe()

# drop one data type columns 
HR.drop(['EmployeeCount', 'EmployeeNumber', 'Over18', 'StandardHours'],
        axis="columns", inplace=True)


print(HR.columns, len(HR.columns))



# =====================================================
# Visualization
# =====================================================



# numerical columns
num_col = []
for i, col in enumerate(HR.columns):
    if HR[col].dtype != object:
        num_col.append(col)
        print("[{}]".format(i+1), end=' ')
        print(col, HR[col].unique())
        print()
        
        
# categorical colmuns 
cate_col = []
for i, col in enumerate(HR.columns):
    if HR[col].dtype == object:
        cate_col.append(col)
        print("[{}]".format(i+1), end=' ')
        print(col, HR[col].unique())
        print()
        

print(HR)



# 종속 변수의 개수가 다르므로 밀도를 이용해서 비교
sns.countplot(HR['Attrition'])

# label encoding
enc = preprocessing.LabelEncoder()
HR["Attrition"] = enc.fit_transform(HR.Attrition)
num_col.append('Attrition')
print(num_col)



# numeric datas
plt.figure(figsize=(40, 40))
for i, column in enumerate(num_col, 1):
    plt.subplot(6, 4, i)
    HR[HR["Attrition"] == 0][column].hist(bins=35, color='green', label='Attrition = NO',alpha=0.5, density=True)
    HR[HR["Attrition"] == 1][column].hist(bins=35, color='red', label='Attrition = YES',alpha=0.5, density=True)
    plt.legend()
    plt.xlabel(column)

plt.tight_layout()
plt.show()


# categorical data
plt.figure(figsize=(30, 30))
for i, column in enumerate(cate_col, 1):
    plt.subplot(2, 4, i)
    HR[HR["Attrition"] == 0][column].hist(bins=35, color='green', label='Attrition = NO',alpha=0.5, density=True)
    HR[HR["Attrition"] == 1][column].hist(bins=35, color='red', label='Attrition = YES',alpha=0.5, density=True)
    plt.legend()
    plt.xlabel(column)


plt.tight_layout()
plt.show()



# =====================================================
# tranform categorical data
# =====================================================

# dummy variable for categorical features
print(pd.get_dummies(HR['MaritalStatus'] ))
marital_dummy = pd.get_dummies(HR['MaritalStatus'])
overTime_dummy =  pd.get_dummies(HR['OverTime'])

print(marital_dummy)
print(overTime_dummy)


for col in HR.columns:
    if HR[col].dtype != object or col == 'Attrition':
        continue
    else:
        HR.drop(col, axis="columns", inplace=True)
    
    
print(HR.columns, len(HR.columns))


HR = HR.join(marital_dummy.add_prefix('Marital_'))
HR = HR.join(overTime_dummy.add_prefix('OverTime_'))
print(HR.columns, len(HR.columns))




# =====================================================
# Corrleation
# =====================================================

# find correlation with pandas ".corr()"
cor = HR.corr() # 상관계수


# visualize with Seaborn heat map, color map = Blues
plt.figure(figsize=(40, 30))
sns.heatmap(abs(cor), annot = False, cmap = plt.cm.Blues)
plt.tight_layout()
plt.show()


# get correlation values with target variable
cor_target = cor["Attrition"]
print(cor_target)
cor_target = abs(cor["Attrition"]) 



# choose features above threshold 0.16

selected_cols = cor_target[cor_target > 0.76]
selected_cols['WorkLifeBalance'] = HR['WorkLifeBalance']
#selected_cols = cor_target[cor_target < 0.05]

print("correlation > 0.16")
print(selected_cols)
#selected_cols = selected_cols.drop('OverTime_No')
selected_cols.index

#스코어 확인용 데이터
selected_cols['Attrition'] = HR['Attrition']


# filter in the selected features
HR_sel = HR[selected_cols.index]
print("\n\n*select features")
print(HR_sel.columns)


    
# =====================================================
# logistic regression
# =====================================================
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

X = HR.drop('Attrition', axis=1)
y = HR['Attrition']

X_train, X_test, y_train, y_test = train_test_split(X, y)

scaler = StandardScaler()
X_train_std = scaler.fit_transform(X_train)
X_test_std = scaler.transform(X_test)
X_std = scaler.transform(X)


model_all = LogisticRegression()
model_all.fit(X_train, y_train)


print("<Logistic Regression Score - All features>\n")
print("train-set", model_all.score(X_train, y_train))
print("test- set", model_all.score(X_test, y_test))


print(model_all.coef_)


#### top 5 featrues
X_sel = HR_sel.drop('Attrition', axis=1)
y_sel = HR_sel['Attrition']

X_train_sel, X_test_sel, y_train_sel, y_test_sel = train_test_split(
    X_sel, y_sel)

"""
sel_scaler = StandardScaler()
X_train_sel_std = sel_scaler.fit_transform(X_train_sel)
X_test_sel_std = sel_scaler.transform(X_test_sel)
X_sel_std = sel_scaler.transform(X_sel)
"""


model_sel = LogisticRegression()
model_sel.fit(X_train_sel, y_train_sel)
print(X_train_sel, y_train_sel)


print("<Logistic Regression Score - 5 features>\n")
print("train-set", model_sel.score(X_train_sel, y_train_sel))
print("test- set", model_sel.score(X_test_sel, y_test_sel))


print("\n\n<weight>")
print(model_sel.coef_)
print(selected_cols)



###############################
# 각 데이터들의 p-value
#############################
x = HR.drop('Attrition', axis=1)
y = HR['Attrition']


from sklearn import metrics
import statsmodels.api as sm
 
log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
 
x2 = sm.add_constant(x)
model = sm.OLS(y, x2)
result = model.fit()
print(result.summary())
 
y_pred = log_reg.predict(X_test)
print(y_pred)
print(list(y_test))
 
print('정확도 :', metrics.accuracy_score(y_test, y_pred))
